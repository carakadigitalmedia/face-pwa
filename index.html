<!doctype html>
<html lang="id">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" />
  <title>Face Detection & Age/Gender — Demo</title>
  <style>
    :root {
      --bg: #0f1724;
      --card: #0b1220;
      --accent: #06b6d4;
      --muted: #94a3b8;
      color-scheme: dark;
    }

    /* Reset & base */
    * {
      box-sizing: border-box;
    }
    body {
      margin: 0;
      font-family: Inter, Segoe UI, Roboto, system-ui, Arial, sans-serif;
      background: linear-gradient(180deg, #071127 0%, #071428 100%);
      color: #e6eef8;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 12px;
      gap: 16px;
    }

    /* Container card */
    .card {
      background: rgba(255, 255, 255, 0.03);
      padding: 16px;
      border-radius: 12px;
      box-shadow: 0 6px 18px rgba(2, 6, 23, 0.6);
      width: 100%;
      max-width: 720px;
    }

    h1 {
      margin: 0 0 12px;
      font-size: 1.5rem;
      font-weight: 600;
      text-align: center;
    }

    /* Controls */
    .controls {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      justify-content: center;
      margin-bottom: 14px;
    }
    button,
    input[type=file] + button {
      padding: 12px 16px;
      border-radius: 10px;
      border: 1px solid rgba(255, 255, 255, 0.1);
      background: transparent;
      color: inherit;
      font-weight: 600;
      font-size: 1rem;
      cursor: pointer;
      transition: background-color 0.25s, color 0.25s;
      min-width: 120px;
      user-select: none;
    }
    button.primary {
      background: var(--accent);
      color: #042029;
      border: none;
    }
    button:hover {
      background: var(--accent);
      color: #042029;
      border: none;
    }

    /* Video container */
    #videoWrap {
      position: relative;
      background: #000;
      border-radius: 12px;
      overflow: hidden;
      width: 100%;
      max-width: 720px;
      aspect-ratio: 16 / 12; /* Maintain ratio for video & canvas */
    }
    video, canvas {
      position: absolute;
      top: 0; left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
      border-radius: 12px;
    }

    /* Log and info */
    .log {
      margin-top: 12px;
      font-size: 14px;
      color: var(--muted);
      text-align: center;
      user-select: none;
    }
    .info {
      background: rgba(255, 255, 255, 0.02);
      padding: 14px 18px;
      border-radius: 10px;
      max-width: 720px;
      width: 100%;
      box-sizing: border-box;
      user-select: none;
    }
    .row {
      display: flex;
      justify-content: space-between;
      padding: 8px 0;
      border-bottom: 1px dashed rgba(255, 255, 255, 0.03);
      font-size: 1rem;
    }

    footer {
      margin-top: 18px;
      font-size: 13px;
      color: var(--muted);
      max-width: 720px;
      width: 100%;
      text-align: center;
      user-select: none;
      padding: 0 12px;
    }

    .badge {
      display: inline-block;
      padding: 6px 12px;
      border-radius: 999px;
      background: #06232a;
      color: #c7fff9;
      font-size: 13px;
      user-select: none;
      align-self: center;
    }

    /* Hide default file input */
    input[type=file] {
      display: none;
    }

    /* Responsive adjustments */
    @media (max-width: 480px) {
      body {
        padding: 12px 6px;
      }
      .card {
        padding: 12px;
        max-width: 100%;
      }
      button, input[type=file] + button {
        min-width: 100%;
        font-size: 1.1rem;
      }
      .controls {
        gap: 8px;
      }
      .row {
        font-size: 1.05rem;
      }
      #videoWrap {
        aspect-ratio: 4 / 3;
        max-width: 100%;
      }
    }
  </style>
</head>
<body>
  <div class="card" role="main" aria-label="Face Detection Application">
    <h1>Face Detection — Age & Gender (client-side)</h1>
    <div class="controls" role="region" aria-label="Control buttons">
      <button id="startCam" class="primary" aria-pressed="false" aria-label="Start Camera">Start Camera</button>
      <button id="stopCam" aria-label="Stop Camera">Stop Camera</button>
      <label>
        <input type="file" id="imageUpload" accept="image/*" aria-label="Upload Image for analysis" />
        <button id="openImage" aria-haspopup="dialog">Open Image</button>
      </label>
      <button id="capture" aria-label="Capture Snapshot">Capture Snapshot</button>
      <button id="download" title="Download overlay as PNG" aria-label="Download Result">Download Result</button>
      <div style="flex:1"></div>
      <div class="badge" aria-live="polite">face-api.js (client)</div>
    </div>

    <div id="videoWrap" aria-live="polite" aria-atomic="true">
      <video id="inputVideo" autoplay muted playsinline></video>
      <canvas id="overlay"></canvas>
    </div>

    <div class="log" id="log">Model status: <strong id="status">Not loaded</strong></div>

    <div class="info" aria-live="polite" aria-atomic="true" style="margin-top:12px">
      <div class="row"><div>Detected faces</div><div id="faceCount">0</div></div>
      <div class="row"><div>Average age (smoothed)</div><div id="avgAge">—</div></div>
      <div class="row"><div>Last result</div><div id="lastResult">—</div></div>
    </div>

    <footer>
      <div>
        Catatan singkat: Aplikasi ini berjalan 100% di browser. Anda harus menyediakan file-model (folder <code>/models</code>) yang berisi model-model dari <code>face-api.js</code> (ageGender, tiny_face_detector, face_landmark_68, face_expression).
      </div>
    </footer>
  </div>

  <script src="https://unpkg.com/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <script>
    const MODEL_URL = './models'
    const video = document.getElementById('inputVideo')
    const canvas = document.getElementById('overlay')
    const startCamBtn = document.getElementById('startCam')
    const stopCamBtn = document.getElementById('stopCam')
    const openImageBtn = document.getElementById('openImage')
    const imageUpload = document.getElementById('imageUpload')
    const captureBtn = document.getElementById('capture')
    const downloadBtn = document.getElementById('download')
    const statusEl = document.getElementById('status')
    const faceCountEl = document.getElementById('faceCount')
    const avgAgeEl = document.getElementById('avgAge')
    const lastResultEl = document.getElementById('lastResult')

    let stream = null
    let detectionInterval = null
    let ageHistory = []
    const AGE_HISTORY_LENGTH = 6

    function pushAge(age){
      ageHistory.push(age)
      if(ageHistory.length > AGE_HISTORY_LENGTH) ageHistory.shift()
      const sum = ageHistory.reduce((a,b)=>a+b,0)
      return sum/ageHistory.length
    }

    async function loadModels(){
      statusEl.textContent = 'Loading models...'
      await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL)
      await faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL)
      await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL)
      await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)
      statusEl.textContent = 'Models loaded'
    }

    function drawDetections(detections){
      const ctx = canvas.getContext('2d')
      ctx.clearRect(0,0,canvas.width,canvas.height)
      detections.forEach(det => {
        const box = det.detection.box
        ctx.strokeStyle = '#06b6d4'
        ctx.lineWidth = Math.max(2, Math.round(box.width/120))
        ctx.strokeRect(box.x, box.y, box.width, box.height)

        const age = det.age
        const gender = det.gender
        const genderProb = det.genderProbability
        const expr = det.expressions
        const topExpression = Object.keys(expr).reduce((a,b)=> expr[a] > expr[b] ? a : b)

        const label = `${Math.round(age)} yrs — ${gender} (${(genderProb*100).toFixed(0)}%) — ${topExpression}`

        ctx.fillStyle = 'rgba(2,6,23,0.7)'
        ctx.fillRect(box.x, box.y - 24, Math.min(360, box.width), 20)
        ctx.fillStyle = '#dffaff'
        ctx.font = '13px Inter, system-ui, Arial'
        ctx.fillText(label, box.x + 6, box.y - 8)
      })
    }

    async function analyzeFrame(input){
      const options = new faceapi.TinyFaceDetectorOptions({inputSize: 512, scoreThreshold: 0.5})
      const results = await faceapi.detectAllFaces(input, options)
        .withAgeAndGender()
        .withFaceLandmarks()
        .withFaceExpressions()

      const annotated = results.map(r => ({
        detection: r.detection,
        age: r.age || 0,
        gender: r.gender || 'unknown',
        genderProbability: r.genderProbability || 0,
        expressions: r.expressions || {}
      }))

           drawDetections(annotated)
      faceCountEl.textContent = annotated.length

      if(annotated.length > 0){
        const ages = annotated.map(a => a.age)
        const avg = pushAge(ages.reduce((s,v) => s + v, 0) / ages.length)
        avgAgeEl.textContent = `${avg.toFixed(1)} years`
        const topExp = Object.keys(annotated[0].expressions).reduce((a,b) =>
          annotated[0].expressions[a] > annotated[0].expressions[b] ? a : b
        )
        lastResultEl.textContent = `${annotated.length} face(s) — ${annotated[0].gender} — ${topExp}`
      } else {
        avgAgeEl.textContent = '—'
        lastResultEl.textContent = 'No faces'
      }
    }

    async function startCamera(){
      if(stream) return
      try{
        stream = await navigator.mediaDevices.getUserMedia({video:{facingMode:'user', width:1280, height:720}, audio:false})
        video.srcObject = stream
        await video.play()
        canvas.width = video.videoWidth
        canvas.height = video.videoHeight
        detectionInterval = setInterval(async () => {
          if(video.readyState < 2) return
          await analyzeFrame(video)
        }, 300)
      } catch(err) {
        alert('Gagal mengakses kamera: ' + err.message)
        console.error(err)
      }
    }

    function stopCamera(){
      if(stream){
        stream.getTracks().forEach(t => t.stop())
        stream = null
      }
      if(detectionInterval) clearInterval(detectionInterval)
      detectionInterval = null
      const ctx = canvas.getContext('2d')
      ctx.clearRect(0,0,canvas.width,canvas.height)
      faceCountEl.textContent = '0'
      avgAgeEl.textContent = '—'
      lastResultEl.textContent = '—'
      video.style.display = ''
      const uploaded = document.getElementById('uploadedPreview')
      if(uploaded) uploaded.remove()
    }

    imageUpload.addEventListener('change', async () => {
      const file = imageUpload.files[0]
      if(!file) return
      const img = await faceapi.bufferToImage(file)
      const ctx = canvas.getContext('2d')
      const maxWidth = 720
      const w = Math.min(maxWidth, img.width)
      const h = Math.round(img.height * (w / img.width))
      canvas.width = w
      canvas.height = h
      const tmp = document.createElement('canvas')
      tmp.width = w
      tmp.height = h
      const tctx = tmp.getContext('2d')
      tctx.drawImage(img, 0, 0, w, h)
      await analyzeFrame(tmp)
      video.pause()
      video.srcObject = null
      video.style.display = 'none'
      const imgEl = document.createElement('img')
      imgEl.src = tmp.toDataURL('image/png')
      imgEl.style.width = '100%'
      imgEl.style.height = '100%'
      imgEl.id = 'uploadedPreview'
      const wrap = document.getElementById('videoWrap')
      const prev = document.getElementById('uploadedPreview')
      if(prev) prev.remove()
      wrap.appendChild(imgEl)
    })

    openImageBtn.addEventListener('click', () => imageUpload.click())

    captureBtn.addEventListener('click', () => {
      const wrap = document.getElementById('videoWrap')
      const out = document.createElement('canvas')
      out.width = canvas.width
      out.height = canvas.height
      const ctx = out.getContext('2d')
      const uploaded = document.getElementById('uploadedPreview')
      if(uploaded){
        ctx.drawImage(uploaded, 0, 0, out.width, out.height)
      } else if(video && video.readyState >= 2){
        ctx.drawImage(video, 0, 0, out.width, out.height)
      } else {
        ctx.fillStyle = '#000'
        ctx.fillRect(0, 0, out.width, out.height)
      }
      ctx.drawImage(canvas, 0, 0)
      const data = out.toDataURL('image/png')
      const w = window.open('about:blank')
      w.document.write(`<img src="${data}" alt="snapshot">`)
    })

    downloadBtn.addEventListener('click', () => {
      const out = document.createElement('canvas')
      out.width = canvas.width
      out.height = canvas.height
      const ctx = out.getContext('2d')
      const uploaded = document.getElementById('uploadedPreview')
      if(uploaded){
        ctx.drawImage(uploaded, 0, 0, out.width, out.height)
      } else if(video && video.readyState >= 2){
        ctx.drawImage(video, 0, 0, out.width, out.height)
      }
      ctx.drawImage(canvas, 0, 0)
      const a = document.createElement('a')
      a.href = out.toDataURL('image/png')
      a.download = 'face_result.png'
      a.click()
    })

    startCamBtn.addEventListener('click', startCamera)
    stopCamBtn.addEventListener('click', stopCamera)

    (async () => {
      try {
        await loadModels()
        const wrap = document.getElementById('videoWrap')
        const rect = wrap.getBoundingClientRect()
        canvas.width = rect.width
        canvas.height = rect.height
        statusEl.textContent = 'Ready — click Start Camera or upload an image'
      } catch(err) {
        statusEl.textContent = 'Model load failed: ' + err.message
        console.error(err)
      }
    })()
  </script>
</body>
</html>
